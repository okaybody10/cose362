{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from kobert_transformers import get_tokenizer, get_kobert_model\n",
    "from gluonnlp.data import SentencepieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 517, 6478, 6079, 6079, 5760, 1409, 5543, 6896, 2582, 517, 0, 5531, 7096, 3100, 54, 3]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "model = get_kobert_model()\n",
    "tok = tokenizer('뽀로로는 남극에 사는 펭귄이 아니다.', padding=True, truncation=True) # Has Input_ids, token_type_ids, attention_mask\n",
    "res = tokenizer.convert_ids_to_tokens(tok['input_ids'])\n",
    "print(tokenizer.convert_tokens_to_ids(res))\n",
    "print(type(tok['attention_mask']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "t = np.random.choice(np.array([2, 3, 4]), size = 2)\n",
    "np.delete(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [2] at entry 0 and [3] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-2a209ed99e08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m# print(torch.sum([torch.LongTensor(i==j) for (i1, j1) in zip(i, j)]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [2] at entry 0 and [3] at entry 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from kobert_transformers import get_kobert_model, get_distilkobert_model\n",
    "from TorchCRF import CRF\n",
    "model = get_kobert_model()\n",
    "model.eval()\n",
    "# input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "attention_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "ans = torch.LongTensor([[0, 1, 2], [2, 3, 4]])\n",
    "xx = model(input_ids, attention_mask, token_type_ids)[0]\n",
    "md = nn.Linear(768, 5)\n",
    "# No mask\n",
    "xxx = md(xx)\n",
    "print(type(xxx))\n",
    "crf = CRF(num_labels=5)\n",
    "i = [torch.LongTensor([1, 2]), torch.LongTensor([2, 3, 4])]\n",
    "j = [torch.LongTensor([2, 2]), torch.LongTensor([1, 3, 5])]\n",
    "# print(torch.sum([torch.LongTensor(i==j) for (i1, j1) in zip(i, j)]))\n",
    "mask = torch.BoolTensor([[1, 0, 1], [1, 1, 1]])\n",
    "print(mask)\n",
    "print(type(crf.viterbi_decode(xxx, mask)), crf.viterbi_decode(xxx, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2, 2287, 6116, 2010, 5782,   54,    3,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1],\n",
      "        [   2,  517, 6478, 6079, 6079, 5760, 1409, 5543, 6896, 2582,  517,    0,\n",
      "         5531, 7096, 3100,   54,    3]])\n",
      "[tensor([   2, 2287, 6116, 2010]), tensor([   2,  517, 6478, 6079, 6079])]\n",
      "tensor([[   2, 2287, 6116, 2010,   -1],\n",
      "        [   2,  517, 6478, 6079, 6079]])\n",
      "last_hidden_state\n",
      "tensor([[[-0.0049,  0.1500,  0.0795,  ..., -0.2515,  0.3697,  0.2850],\n",
      "         [ 0.0881,  0.2531,  0.4122,  ..., -0.5684, -0.2381,  0.2486],\n",
      "         [ 0.1131, -0.0638,  0.2878,  ...,  0.0902, -0.5898,  0.2046],\n",
      "         ...,\n",
      "         [-0.0308, -0.0373,  0.0136,  ..., -0.1681,  0.0115, -0.0494],\n",
      "         [-0.0308, -0.0373,  0.0136,  ..., -0.1681,  0.0115, -0.0494],\n",
      "         [-0.0308, -0.0373,  0.0136,  ..., -0.1681,  0.0115, -0.0494]],\n",
      "\n",
      "        [[ 0.1627, -0.0937,  0.0815,  ...,  0.2031,  0.2887, -0.0510],\n",
      "         [ 0.0094, -0.1836,  0.2938,  ..., -0.2896, -0.1239, -0.4415],\n",
      "         [ 0.2271, -0.4674, -0.0282,  ...,  0.3514, -0.0007,  0.2542],\n",
      "         ...,\n",
      "         [-0.1576,  0.0680,  0.3799,  ..., -0.3053,  0.3324, -0.0062],\n",
      "         [-0.0140, -0.0956,  0.2706,  ..., -0.3518,  0.4278, -0.3793],\n",
      "         [ 0.1321, -0.0879,  0.3344,  ..., -0.0410,  0.4205, -0.3081]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "model.eval()\n",
    "res = tokenizer(['배를 먹다.', '뽀로로는 남극에 사는 펭귄이 아니다.'], padding=True, truncation=True, return_tensors='pt')\n",
    "print(res['input_ids'])\n",
    "texts = []\n",
    "for idx, i in enumerate(res['input_ids']) :\n",
    "    texts.append(i[:idx+4])\n",
    "# texts = [res[0]['input_ids'][:3], res[1]['input_ids'][:5]]\n",
    "print(texts)\n",
    "texts = pad_sequence(texts, batch_first=True, padding_value= -1)\n",
    "print(texts)\n",
    "seq, pooled_output  = model(**res)\n",
    "print(seq)\n",
    "print(model(**res)[seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [2, 4], 2: [4, 7]}\n"
     ]
    }
   ],
   "source": [
    "aa = [{1: 2, 2:4}, {1: 4, 2: 7}]\n",
    "print({key: [i[key] for i in aa] for key in aa[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-PS': 1, 'B-FD': 2, 'B-TR': 3, 'B-AF': 4, 'B-OG': 5, 'B-LC': 6, 'B-CV': 7, 'B-DT': 8, 'B-TI': 10, 'B-QT': 11, 'B-EV': 12, 'B-AM': 13, 'B-PT': 14, 'B-MT': 15, 'B-TM': 16, 'I-PS': 17, 'I-FD': 18, 'I-TR': 19, 'I-AF': 20, 'I-OG': 21, 'I-LC': 22, 'I-CV': 23, 'I-DT': 24, 'I-TI': 26, 'I-QT': 27, 'I-EV': 28, 'I-AM': 29, 'I-PT': 30, 'I-MT': 31, 'I-TM': 32} {0: 'O', 1: 'B-PS', 2: 'B-FD', 3: 'B-TR', 4: 'B-AF', 5: 'B-OG', 6: 'B-LC', 7: 'B-CV', 8: 'B-DT', 9: 'B-TI', 10: 'B-TI', 11: 'B-QT', 12: 'B-EV', 13: 'B-AM', 14: 'B-PT', 15: 'B-MT', 16: 'B-TM', 17: 'I-PS', 18: 'I-FD', 19: 'I-TR', 20: 'I-AF', 21: 'I-OG', 22: 'I-LC', 23: 'I-CV', 24: 'I-DT', 25: 'I-TI', 26: 'I-TI', 27: 'I-QT', 28: 'I-EV', 29: 'I-AM', 30: 'I-PT', 31: 'I-MT', 32: 'I-TM'}\n"
     ]
    }
   ],
   "source": [
    "label_list = ['PS', 'FD', 'TR', 'AF', 'OG', 'LC', 'CV', 'DT', 'TI', 'TI', 'QT', 'EV', 'AM', 'PT', 'MT', \"TM\"] \n",
    "label_fin = ['O']\n",
    "label_fin += ['B-' + i for i in label_list]\n",
    "label_fin += ['I-' + i for i in label_list]\n",
    "label_to_idx = {label: idx for idx, label in enumerate(label_fin)}\n",
    "idx_to_label = {idx: label for idx, label in enumerate(label_fin)}\n",
    "print(label_to_idx, idx_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We will return the label of given words, using the ne_lists\n",
    "We use BIO-tagging\n",
    "'''\n",
    "def tagging(words: List[str], ne_lists: List[Dict[str, Any]]) -> List[str] :\n",
    "    results = [i if i in ['[CLS]', '[SEP]', '[PAD]'] else 'O' for i in words] # If token is not Special, initialize 'O' tag\n",
    "    ps_words = [i.replace('##', '').replace('▁','') for i in words]\n",
    "    ne_cnt = len(ne_lists)\n",
    "    ne_idx = -1\n",
    "    ne_label = 0\n",
    "\n",
    "    for idx, word in enumerate(ps_words) :\n",
    "        if results[idx] != 'O' or word == '' or word == '[UNK]':\n",
    "            continue\n",
    "        if word == '[UNK]' :\n",
    "            continue\n",
    "        # Now condition check\n",
    "        if ne_idx >= 0 : \n",
    "            nw_word = ne_lists[ne_idx]['form'][ne_label:]\n",
    "        else :\n",
    "            nw_word = ''\n",
    "\n",
    "        # I-tag condition\n",
    "        if (len(nw_word) > 0) & (nw_word.startswith(word)) & (results[idx-1][0] == 'B' or results[idx-1][0] == 'I') :\n",
    "            results[idx] = 'I-' + ne_lists[ne_idx]['label'][:2]\n",
    "            ne_label += len(word)\n",
    "        else : # B-tag condition\n",
    "            back_idx = ne_idx\n",
    "            back_label = ne_label\n",
    "            while ne_idx + 1 < ne_cnt :\n",
    "                ne_idx += 1\n",
    "                ne_label = 0\n",
    "                nw_word = ne_lists[ne_idx]['form']\n",
    "                if (len(nw_word) > 0) & (nw_word.startswith(word)) :\n",
    "                    results[idx] = 'B-' + ne_lists[ne_idx]['label'][:2]\n",
    "                    ne_label += len(word)\n",
    "                    break\n",
    "            if ne_idx + 1 == ne_cnt and ne_label == 0:\n",
    "                ne_idx = back_idx\n",
    "                ne_label = back_label\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁태', '안', '군의', '회', ',', '▁20', '19', '년', '‘', '군', '민', '중심', '’', '의', '정', '성과', '▁빛', '났다', '!', '[SEP]']\n",
      "['[CLS]', 'B-OG', 'I-OG', 'I-OG', 'I-OG', 'O', 'B-DT', 'I-DT', 'I-DT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"태안군의회, 2019년‘군민중심’의정성과 빛났다!\"\n",
    "ne = [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"form\": \"태안군의회\",\n",
    "            \"label\": \"OGG_POLITICS\",\n",
    "            \"begin\": 0,\n",
    "            \"end\": 5\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"form\": \"2019년\",\n",
    "            \"label\": \"DT_YEAR\",\n",
    "            \"begin\": 7,\n",
    "            \"end\": 12\n",
    "        }\n",
    "]\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "tok = tokenizer(sentence, padding=True, truncation=True) # Has Input_ids, token_type_ids, attention_mask\n",
    "tokens_word = tokenizer.convert_ids_to_tokens(tok['input_ids'])\n",
    "print(tokens_word, tagging(tokens_word, ne), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Json loads & dataframe preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_files(path='./dataset/NLNE2202211219.json') :\n",
    "    with open(path, \"r\") as f :\n",
    "        bef_data = json.load(f)\n",
    "\n",
    "    bef_data = bef_data['document']\n",
    "\n",
    "    df_tot = pd.DataFrame(columns=['form', 'NE'])\n",
    "\n",
    "    for r in bef_data :\n",
    "        df_tot = df_tot.append(pd.DataFrame.from_records(r['sentence'], columns=['form', 'NE']))\n",
    "\n",
    "    df_tot.dropna(how='any', inplace=True)\n",
    "\n",
    "    return df_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_df(data: List[Any]) -> pd.DataFrame:\n",
    "#     return pd.DataFrame.from_records(data['sentence'], columns=['form'])\n",
    "# ex = bef_data[0]\n",
    "# ex = ex['sentence']\n",
    "# print(type(ex), ex[1], sep='\\n')\n",
    "# df = pd.DataFrame.from_records(ex, columns=['form', 'NE'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Define DataLoader, with tokenizer\n",
    "# Have to define collect_fn, to gather attention mask and another information\n",
    "# tok = tokenizer('뽀로로는 남극에 사는 펭귄이 아니다.', padding=True, truncation=True) # Has Input_ids, token_type_ids, attention_mask\n",
    "# tokenizer.convert_ids_to_tokens(tok['input_ids'])\n",
    "df = load_files()\n",
    "texts = df['form'].to_list()\n",
    "ne = df['NE'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset) :\n",
    "    def __init__(self, texts, labels, tokenizer, max_len = 256) -> None:\n",
    "        self.tokenizer = tokenizer \n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index) -> Any:\n",
    "        # tokenizer\n",
    "        input = self.texts[index]\n",
    "        sentence = self.tokenizer(input, padding = 'max_length', truncation = True, max_length = self.max_len) # Input_ids, token_type_ids, attention_mask\n",
    "        tags = tagging(self.tokenizer.convert_ids_to_tokens(sentence['input_ids']), self.labels[index])\n",
    "        convert_tags = [-100 if i in ['[CLS]', '[SEP]', '[PAD]'] else label_to_idx[i] for i in tags]\n",
    "        return {\n",
    "            'sentence' : input, # str\n",
    "            'input_ids' : torch.tensor(sentence['input_ids'], dtype=torch.long).to(devices),\n",
    "            'token_type_id' : torch.tensor(sentence['token_type_ids'], dtype=torch.long).to(devices),\n",
    "            'attention_mask' : torch.tensor(sentence['attention_mask'], dtype=torch.long).to(devices),\n",
    "            'labels' : torch.tensor(convert_tags, dtype=torch.long).to(devices)\n",
    "        } # Collect_fn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, test_texts, train_ne, test_ne = train_test_split(texts, ne, test_size=0.2, random_state=42)\n",
    "test_dataset = CustomDataset(test_texts, test_ne, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sung/anaconda3/envs/cose362/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 2 at dim 1 (got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c0a29952d5eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 2 at dim 1 (got 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1, 2], [3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "32\n",
      "5\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for batch in test_loader :\n",
    "    if cnt > 1 :\n",
    "        break\n",
    "    cnt += 1\n",
    "    print(len(batch), len(batch['sentence']), sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_implement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
