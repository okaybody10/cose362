{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sung/anaconda3/envs/cose362/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from kobert_transformers import get_tokenizer\n",
    "from gluonnlp.data import SentencepieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = tokenizer('뽀로로는 남극에 사는 펭귄이 아니다.', padding=True, truncation=True) # Has Input_ids, token_type_ids, attention_mask\n",
    "res = tokenizer.convert_ids_to_tokens(tok['input_ids'])\n",
    "print(type(tok['attention_mask']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['PS', 'FD', 'TR', 'AF', 'OG', 'LC', 'CV', 'DT', 'TI', 'TI', 'QT', 'EV', 'AM', 'PT', 'MT', \"TM\"] \n",
    "label_fin = ['O']\n",
    "label_fin += ['B-' + i for i in label_list]\n",
    "label_fin += ['I-' + i for i in label_list]\n",
    "label_to_idx = {label: idx for idx, label in enumerate(label_fin)}\n",
    "idx_to_label = {idx: label for idx, label in enumerate(label_fin)}\n",
    "print(label_to_idx, idx_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We will return the label of given words, using the ne_lists\n",
    "We use BIO-tagging\n",
    "'''\n",
    "def tagging(words: List[str], ne_lists: List[Dict[str, Any]]) -> List[str] :\n",
    "    results = [i if i in ['[CLS]', '[SEP]', '[PAD]'] else 'O' for i in words] # If token is not Special, initialize 'O' tag\n",
    "    ps_words = [i.replace('##', '').replace('▁','') for i in words]\n",
    "    ne_cnt = len(ne_lists)\n",
    "    ne_idx = -1\n",
    "    ne_label = 0\n",
    "\n",
    "    for idx, word in enumerate(ps_words) :\n",
    "        if results[idx] != 'O' or word == '' or word == '[UNK]':\n",
    "            continue\n",
    "        if word == '[UNK]' :\n",
    "            continue\n",
    "        # Now condition check\n",
    "        if ne_idx >= 0 : \n",
    "            nw_word = ne_lists[ne_idx]['form'][ne_label:]\n",
    "        else :\n",
    "            nw_word = ''\n",
    "\n",
    "        # I-tag condition\n",
    "        if (len(nw_word) > 0) & (nw_word.startswith(word)) & (results[idx-1][0] == 'B' or results[idx-1][0] == 'I') :\n",
    "            results[idx] = 'I-' + ne_lists[ne_idx]['label'][:2]\n",
    "            ne_label += len(word)\n",
    "        else : # B-tag condition\n",
    "            back_idx = ne_idx\n",
    "            back_label = ne_label\n",
    "            while ne_idx + 1 < ne_cnt :\n",
    "                ne_idx += 1\n",
    "                ne_label = 0\n",
    "                nw_word = ne_lists[ne_idx]['form']\n",
    "                if (len(nw_word) > 0) & (nw_word.startswith(word)) :\n",
    "                    results[idx] = 'B-' + ne_lists[ne_idx]['label'][:2]\n",
    "                    ne_label += len(word)\n",
    "                    break\n",
    "            if ne_idx + 1 == ne_cnt and ne_label == 0:\n",
    "                ne_idx = back_idx\n",
    "                ne_label = back_label\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"태안군의회, 2019년‘군민중심’의정성과 빛났다!\"\n",
    "ne = [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"form\": \"태안군의회\",\n",
    "            \"label\": \"OGG_POLITICS\",\n",
    "            \"begin\": 0,\n",
    "            \"end\": 5\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"form\": \"2019년\",\n",
    "            \"label\": \"DT_YEAR\",\n",
    "            \"begin\": 7,\n",
    "            \"end\": 12\n",
    "        }\n",
    "]\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "tok = tokenizer(sentence, padding=True, truncation=True) # Has Input_ids, token_type_ids, attention_mask\n",
    "tokens_word = tokenizer.convert_ids_to_tokens(tok['input_ids'])\n",
    "print(tokens_word, tagging(tokens_word, ne), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Json loads & dataframe preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_files(path='./dataset/NLNE2202211219.json') :\n",
    "    with open(path, \"r\") as f :\n",
    "        bef_data = json.load(f)\n",
    "\n",
    "    bef_data = bef_data['document']\n",
    "\n",
    "    df_tot = pd.DataFrame(columns=['form', 'NE'])\n",
    "\n",
    "    for r in bef_data :\n",
    "        df_tot = df_tot.append(pd.DataFrame.from_records(r['sentence'], columns=['form', 'NE']))\n",
    "\n",
    "    df_tot.dropna(how='any', inplace=True)\n",
    "\n",
    "    return df_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = load_files()\n",
    "tt.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_df(data: List[Any]) -> pd.DataFrame:\n",
    "#     return pd.DataFrame.from_records(data['sentence'], columns=['form'])\n",
    "# ex = bef_data[0]\n",
    "# ex = ex['sentence']\n",
    "# print(type(ex), ex[1], sep='\\n')\n",
    "# df = pd.DataFrame.from_records(ex, columns=['form', 'NE'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define DataLoader, with tokenizer\n",
    "# Have to define collect_fn, to gather attention mask and another information\n",
    "# tok = tokenizer('뽀로로는 남극에 사는 펭귄이 아니다.', padding=True, truncation=True) # Has Input_ids, token_type_ids, attention_mask\n",
    "# tokenizer.convert_ids_to_tokens(tok['input_ids'])\n",
    "df = load_files()\n",
    "texts = df['form']\n",
    "ne = df['NE']\n",
    "\n",
    "class CustomDataset(Dataset) :\n",
    "    def __init__(self, texts, labels, tokenizer, max_len) -> None:\n",
    "        self.tokenizer = tokenizer \n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index) -> Any:\n",
    "        # tokenizer\n",
    "        input = self.texts[index]\n",
    "        sentence = self.tokenizer(input, max_len = self.max_len, padding = True, truncation = True) # Input_ids, token_type_ids, attention_mask\n",
    "        tags = tagging(self.tokenizer.convert_ids_to_tokens(sentence['input_ids']))\n",
    "        return {\n",
    "            'sentence' : input,\n",
    "            'input_ids' : sentence['input_ids'],\n",
    "            'token_type_id' : sentence['token_type_ids'],\n",
    "            'attention_mask' : sentence['attention_mask'],\n",
    "            'labels' : tags\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_implement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
