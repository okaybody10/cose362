{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from kobert_transformers import get_tokenizer, get_kobert_model\n",
    "from gluonnlp.data import SentencepieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 517, 6478, 6079, 6079, 5760, 1409, 5543, 6896, 2582, 517, 0, 5531, 7096, 3100, 54, 3]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "model = get_kobert_model()\n",
    "tok = tokenizer('뽀로로는 남극에 사는 펭귄이 아니다.', padding=True, truncation=True) # Has Input_ids, token_type_ids, attention_mask\n",
    "res = tokenizer.convert_ids_to_tokens(tok['input_ids'])\n",
    "print(tokenizer.convert_tokens_to_ids(res))\n",
    "print(type(tok['attention_mask']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "t = np.random.choice(np.array([2, 3, 4]), size = 2)\n",
    "np.delete(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2, 2287, 6116, 2010, 5782,   54,    3,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1],\n",
      "        [   2,  517, 6478, 6079, 6079, 5760, 1409, 5543, 6896, 2582,  517,    0,\n",
      "         5531, 7096, 3100,   54,    3]])\n",
      "[tensor([   2, 2287, 6116, 2010]), tensor([   2,  517, 6478, 6079, 6079])]\n",
      "tensor([[   2, 2287, 6116, 2010,   -1],\n",
      "        [   2,  517, 6478, 6079, 6079]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "where() received an invalid combination of arguments - got (Tensor, str, Tensor), but expected one of:\n * (Tensor condition)\n * (Tensor condition, Tensor input, Tensor other)\n      didn't match because some of the arguments have invalid types: (Tensor, !str!, Tensor)\n * (Tensor condition, Number self, Tensor other)\n      didn't match because some of the arguments have invalid types: (Tensor, !str!, Tensor)\n * (Tensor condition, Tensor input, Number other)\n      didn't match because some of the arguments have invalid types: (Tensor, !str!, !Tensor!)\n * (Tensor condition, Number self, Number other)\n      didn't match because some of the arguments have invalid types: (Tensor, !str!, !Tensor!)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-51454181ca1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_value\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'[PAD]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: where() received an invalid combination of arguments - got (Tensor, str, Tensor), but expected one of:\n * (Tensor condition)\n * (Tensor condition, Tensor input, Tensor other)\n      didn't match because some of the arguments have invalid types: (Tensor, !str!, Tensor)\n * (Tensor condition, Number self, Tensor other)\n      didn't match because some of the arguments have invalid types: (Tensor, !str!, Tensor)\n * (Tensor condition, Tensor input, Number other)\n      didn't match because some of the arguments have invalid types: (Tensor, !str!, !Tensor!)\n * (Tensor condition, Number self, Number other)\n      didn't match because some of the arguments have invalid types: (Tensor, !str!, !Tensor!)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "res = tokenizer(['배를 먹다.', '뽀로로는 남극에 사는 펭귄이 아니다.'], padding=True, truncation=True, return_tensors='pt')\n",
    "print(res['input_ids'])\n",
    "texts = []\n",
    "for idx, i in enumerate(res['input_ids']) :\n",
    "    texts.append(i[:idx+4])\n",
    "# texts = [res[0]['input_ids'][:3], res[1]['input_ids'][:5]]\n",
    "print(texts)\n",
    "texts = pad_sequence(texts, batch_first=True, padding_value= -1)\n",
    "print(texts)\n",
    "texts = torch.where(texts == -1, '[PAD]', texts)\n",
    "print(texts)\n",
    "model.eval()\n",
    "print(model.embeddings.word_embeddings(torch.tensor(res['input_ids'])), model(**res)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [2, 4], 2: [4, 7]}\n"
     ]
    }
   ],
   "source": [
    "aa = [{1: 2, 2:4}, {1: 4, 2: 7}]\n",
    "print({key: [i[key] for i in aa] for key in aa[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-PS': 1, 'B-FD': 2, 'B-TR': 3, 'B-AF': 4, 'B-OG': 5, 'B-LC': 6, 'B-CV': 7, 'B-DT': 8, 'B-TI': 10, 'B-QT': 11, 'B-EV': 12, 'B-AM': 13, 'B-PT': 14, 'B-MT': 15, 'B-TM': 16, 'I-PS': 17, 'I-FD': 18, 'I-TR': 19, 'I-AF': 20, 'I-OG': 21, 'I-LC': 22, 'I-CV': 23, 'I-DT': 24, 'I-TI': 26, 'I-QT': 27, 'I-EV': 28, 'I-AM': 29, 'I-PT': 30, 'I-MT': 31, 'I-TM': 32} {0: 'O', 1: 'B-PS', 2: 'B-FD', 3: 'B-TR', 4: 'B-AF', 5: 'B-OG', 6: 'B-LC', 7: 'B-CV', 8: 'B-DT', 9: 'B-TI', 10: 'B-TI', 11: 'B-QT', 12: 'B-EV', 13: 'B-AM', 14: 'B-PT', 15: 'B-MT', 16: 'B-TM', 17: 'I-PS', 18: 'I-FD', 19: 'I-TR', 20: 'I-AF', 21: 'I-OG', 22: 'I-LC', 23: 'I-CV', 24: 'I-DT', 25: 'I-TI', 26: 'I-TI', 27: 'I-QT', 28: 'I-EV', 29: 'I-AM', 30: 'I-PT', 31: 'I-MT', 32: 'I-TM'}\n"
     ]
    }
   ],
   "source": [
    "label_list = ['PS', 'FD', 'TR', 'AF', 'OG', 'LC', 'CV', 'DT', 'TI', 'TI', 'QT', 'EV', 'AM', 'PT', 'MT', \"TM\"] \n",
    "label_fin = ['O']\n",
    "label_fin += ['B-' + i for i in label_list]\n",
    "label_fin += ['I-' + i for i in label_list]\n",
    "label_to_idx = {label: idx for idx, label in enumerate(label_fin)}\n",
    "idx_to_label = {idx: label for idx, label in enumerate(label_fin)}\n",
    "print(label_to_idx, idx_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We will return the label of given words, using the ne_lists\n",
    "We use BIO-tagging\n",
    "'''\n",
    "def tagging(words: List[str], ne_lists: List[Dict[str, Any]]) -> List[str] :\n",
    "    results = [i if i in ['[CLS]', '[SEP]', '[PAD]'] else 'O' for i in words] # If token is not Special, initialize 'O' tag\n",
    "    ps_words = [i.replace('##', '').replace('▁','') for i in words]\n",
    "    ne_cnt = len(ne_lists)\n",
    "    ne_idx = -1\n",
    "    ne_label = 0\n",
    "\n",
    "    for idx, word in enumerate(ps_words) :\n",
    "        if results[idx] != 'O' or word == '' or word == '[UNK]':\n",
    "            continue\n",
    "        if word == '[UNK]' :\n",
    "            continue\n",
    "        # Now condition check\n",
    "        if ne_idx >= 0 : \n",
    "            nw_word = ne_lists[ne_idx]['form'][ne_label:]\n",
    "        else :\n",
    "            nw_word = ''\n",
    "\n",
    "        # I-tag condition\n",
    "        if (len(nw_word) > 0) & (nw_word.startswith(word)) & (results[idx-1][0] == 'B' or results[idx-1][0] == 'I') :\n",
    "            results[idx] = 'I-' + ne_lists[ne_idx]['label'][:2]\n",
    "            ne_label += len(word)\n",
    "        else : # B-tag condition\n",
    "            back_idx = ne_idx\n",
    "            back_label = ne_label\n",
    "            while ne_idx + 1 < ne_cnt :\n",
    "                ne_idx += 1\n",
    "                ne_label = 0\n",
    "                nw_word = ne_lists[ne_idx]['form']\n",
    "                if (len(nw_word) > 0) & (nw_word.startswith(word)) :\n",
    "                    results[idx] = 'B-' + ne_lists[ne_idx]['label'][:2]\n",
    "                    ne_label += len(word)\n",
    "                    break\n",
    "            if ne_idx + 1 == ne_cnt and ne_label == 0:\n",
    "                ne_idx = back_idx\n",
    "                ne_label = back_label\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁태', '안', '군의', '회', ',', '▁20', '19', '년', '‘', '군', '민', '중심', '’', '의', '정', '성과', '▁빛', '났다', '!', '[SEP]']\n",
      "['[CLS]', 'B-OG', 'I-OG', 'I-OG', 'I-OG', 'O', 'B-DT', 'I-DT', 'I-DT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"태안군의회, 2019년‘군민중심’의정성과 빛났다!\"\n",
    "ne = [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"form\": \"태안군의회\",\n",
    "            \"label\": \"OGG_POLITICS\",\n",
    "            \"begin\": 0,\n",
    "            \"end\": 5\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"form\": \"2019년\",\n",
    "            \"label\": \"DT_YEAR\",\n",
    "            \"begin\": 7,\n",
    "            \"end\": 12\n",
    "        }\n",
    "]\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "tok = tokenizer(sentence, padding=True, truncation=True) # Has Input_ids, token_type_ids, attention_mask\n",
    "tokens_word = tokenizer.convert_ids_to_tokens(tok['input_ids'])\n",
    "print(tokens_word, tagging(tokens_word, ne), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Json loads & dataframe preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_files(path='./dataset/NLNE2202211219.json') :\n",
    "    with open(path, \"r\") as f :\n",
    "        bef_data = json.load(f)\n",
    "\n",
    "    bef_data = bef_data['document']\n",
    "\n",
    "    df_tot = pd.DataFrame(columns=['form', 'NE'])\n",
    "\n",
    "    for r in bef_data :\n",
    "        df_tot = df_tot.append(pd.DataFrame.from_records(r['sentence'], columns=['form', 'NE']))\n",
    "\n",
    "    df_tot.dropna(how='any', inplace=True)\n",
    "\n",
    "    return df_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_df(data: List[Any]) -> pd.DataFrame:\n",
    "#     return pd.DataFrame.from_records(data['sentence'], columns=['form'])\n",
    "# ex = bef_data[0]\n",
    "# ex = ex['sentence']\n",
    "# print(type(ex), ex[1], sep='\\n')\n",
    "# df = pd.DataFrame.from_records(ex, columns=['form', 'NE'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Define DataLoader, with tokenizer\n",
    "# Have to define collect_fn, to gather attention mask and another information\n",
    "# tok = tokenizer('뽀로로는 남극에 사는 펭귄이 아니다.', padding=True, truncation=True) # Has Input_ids, token_type_ids, attention_mask\n",
    "# tokenizer.convert_ids_to_tokens(tok['input_ids'])\n",
    "df = load_files()\n",
    "texts = df['form'].to_list()\n",
    "ne = df['NE'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset) :\n",
    "    def __init__(self, texts, labels, tokenizer, max_len = 256) -> None:\n",
    "        self.tokenizer = tokenizer \n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index) -> Any:\n",
    "        # tokenizer\n",
    "        input = self.texts[index]\n",
    "        sentence = self.tokenizer(input, padding = 'max_length', truncation = True, max_length = self.max_len) # Input_ids, token_type_ids, attention_mask\n",
    "        tags = tagging(self.tokenizer.convert_ids_to_tokens(sentence['input_ids']), self.labels[index])\n",
    "        convert_tags = [-100 if i in ['[CLS]', '[SEP]', '[PAD]'] else label_to_idx[i] for i in tags]\n",
    "        return {\n",
    "            'sentence' : input, # str\n",
    "            'input_ids' : torch.tensor(sentence['input_ids'], dtype=torch.long).to(devices),\n",
    "            'token_type_id' : torch.tensor(sentence['token_type_ids'], dtype=torch.long).to(devices),\n",
    "            'attention_mask' : torch.tensor(sentence['attention_mask'], dtype=torch.long).to(devices),\n",
    "            'labels' : torch.tensor(convert_tags, dtype=torch.long).to(devices)\n",
    "        } # Collect_fn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, test_texts, train_ne, test_ne = train_test_split(texts, ne, test_size=0.2, random_state=42)\n",
    "test_dataset = CustomDataset(test_texts, test_ne, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sung/anaconda3/envs/cose362/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 2 at dim 1 (got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c0a29952d5eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 2 at dim 1 (got 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1, 2], [3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "32\n",
      "5\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for batch in test_loader :\n",
    "    if cnt > 1 :\n",
    "        break\n",
    "    cnt += 1\n",
    "    print(len(batch), len(batch['sentence']), sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_implement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
