{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from kobert_transformers import get_tokenizer, get_kobert_model\n",
    "from gluonnlp.data import SentencepieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 426/426 [00:00<00:00, 345kB/s]\n",
      "Downloading: 100%|██████████| 369M/369M [00:11<00:00, 32.6MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "model = get_kobert_model()\n",
    "tok = tokenizer('뽀로로는 남극에 사는 펭귄이 아니다.', padding=True, truncation=True) # Has Input_ids, token_type_ids, attention_mask\n",
    "res = tokenizer.convert_ids_to_tokens(tok['input_ids'])\n",
    "print(type(tok['attention_mask']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   2, 2287, 6116, 2010, 5782,   54,    3],\n",
      "        [   2, 2287, 6116, 3520, 7798,   54,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[[ 0.0035, -0.0004, -0.0004,  ...,  0.0031,  0.0103, -0.0033],\n",
      "         [-0.0612,  0.0943, -0.0233,  ..., -0.0226, -0.0094,  0.0006],\n",
      "         [ 0.0036,  0.0507, -0.0636,  ...,  0.0219,  0.0400, -0.0527],\n",
      "         ...,\n",
      "         [-0.0107, -0.0162,  0.0421,  ..., -0.0337,  0.0868,  0.0350],\n",
      "         [-0.0103, -0.0228,  0.0168,  ..., -0.0711,  0.0695, -0.0155],\n",
      "         [ 0.0723, -0.0379,  0.0271,  ..., -0.0282,  0.0169, -0.0818]],\n",
      "\n",
      "        [[ 0.0035, -0.0004, -0.0004,  ...,  0.0031,  0.0103, -0.0033],\n",
      "         [-0.0612,  0.0943, -0.0233,  ..., -0.0226, -0.0094,  0.0006],\n",
      "         [ 0.0036,  0.0507, -0.0636,  ...,  0.0219,  0.0400, -0.0527],\n",
      "         ...,\n",
      "         [-0.0463,  0.0422, -0.0984,  ..., -0.0517,  0.0543, -0.0093],\n",
      "         [-0.0103, -0.0228,  0.0168,  ..., -0.0711,  0.0695, -0.0155],\n",
      "         [ 0.0723, -0.0379,  0.0271,  ..., -0.0282,  0.0169, -0.0818]]],\n",
      "       grad_fn=<EmbeddingBackward0>) tensor([[[-0.0049,  0.1500,  0.0795,  ..., -0.2515,  0.3697,  0.2850],\n",
      "         [ 0.0881,  0.2531,  0.4122,  ..., -0.5684, -0.2381,  0.2486],\n",
      "         [ 0.1131, -0.0638,  0.2878,  ...,  0.0902, -0.5898,  0.2046],\n",
      "         ...,\n",
      "         [-0.0145, -0.1863,  0.3254,  ..., -0.2621,  0.4775, -0.1444],\n",
      "         [-0.2091, -0.0330,  0.0966,  ..., -0.5195,  0.4986, -0.1204],\n",
      "         [-0.0307, -0.0373,  0.0136,  ..., -0.1682,  0.0115, -0.0493]],\n",
      "\n",
      "        [[-0.2350,  0.1317,  0.3062,  ..., -0.2119,  0.1032,  0.0049],\n",
      "         [ 0.0107,  0.2707,  0.0850,  ..., -0.3715, -0.2527,  0.1569],\n",
      "         [ 0.0869, -0.1778,  0.3050,  ...,  0.3987, -0.7045, -0.0401],\n",
      "         ...,\n",
      "         [-0.3633,  0.1335, -0.0400,  ..., -0.1275,  0.4040,  0.0119],\n",
      "         [-0.3118, -0.0040,  0.1896,  ..., -0.3993,  0.3148, -0.2047],\n",
      "         [-0.0389, -0.0060,  0.0469,  ...,  0.0488, -0.2084, -0.1967]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "res = tokenizer(['배를 먹다.', '배를 운전하다.'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(res)\n",
    "model.eval()\n",
    "print(model.embeddings.word_embeddings(res['input_ids']), model(**res)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [2, 4], 2: [4, 7]}\n"
     ]
    }
   ],
   "source": [
    "aa = [{1: 2, 2:4}, {1: 4, 2: 7}]\n",
    "print({key: [i[key] for i in aa] for key in aa[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-PS': 1, 'B-FD': 2, 'B-TR': 3, 'B-AF': 4, 'B-OG': 5, 'B-LC': 6, 'B-CV': 7, 'B-DT': 8, 'B-TI': 10, 'B-QT': 11, 'B-EV': 12, 'B-AM': 13, 'B-PT': 14, 'B-MT': 15, 'B-TM': 16, 'I-PS': 17, 'I-FD': 18, 'I-TR': 19, 'I-AF': 20, 'I-OG': 21, 'I-LC': 22, 'I-CV': 23, 'I-DT': 24, 'I-TI': 26, 'I-QT': 27, 'I-EV': 28, 'I-AM': 29, 'I-PT': 30, 'I-MT': 31, 'I-TM': 32} {0: 'O', 1: 'B-PS', 2: 'B-FD', 3: 'B-TR', 4: 'B-AF', 5: 'B-OG', 6: 'B-LC', 7: 'B-CV', 8: 'B-DT', 9: 'B-TI', 10: 'B-TI', 11: 'B-QT', 12: 'B-EV', 13: 'B-AM', 14: 'B-PT', 15: 'B-MT', 16: 'B-TM', 17: 'I-PS', 18: 'I-FD', 19: 'I-TR', 20: 'I-AF', 21: 'I-OG', 22: 'I-LC', 23: 'I-CV', 24: 'I-DT', 25: 'I-TI', 26: 'I-TI', 27: 'I-QT', 28: 'I-EV', 29: 'I-AM', 30: 'I-PT', 31: 'I-MT', 32: 'I-TM'}\n"
     ]
    }
   ],
   "source": [
    "label_list = ['PS', 'FD', 'TR', 'AF', 'OG', 'LC', 'CV', 'DT', 'TI', 'TI', 'QT', 'EV', 'AM', 'PT', 'MT', \"TM\"] \n",
    "label_fin = ['O']\n",
    "label_fin += ['B-' + i for i in label_list]\n",
    "label_fin += ['I-' + i for i in label_list]\n",
    "label_to_idx = {label: idx for idx, label in enumerate(label_fin)}\n",
    "idx_to_label = {idx: label for idx, label in enumerate(label_fin)}\n",
    "print(label_to_idx, idx_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We will return the label of given words, using the ne_lists\n",
    "We use BIO-tagging\n",
    "'''\n",
    "def tagging(words: List[str], ne_lists: List[Dict[str, Any]]) -> List[str] :\n",
    "    results = [i if i in ['[CLS]', '[SEP]', '[PAD]'] else 'O' for i in words] # If token is not Special, initialize 'O' tag\n",
    "    ps_words = [i.replace('##', '').replace('▁','') for i in words]\n",
    "    ne_cnt = len(ne_lists)\n",
    "    ne_idx = -1\n",
    "    ne_label = 0\n",
    "\n",
    "    for idx, word in enumerate(ps_words) :\n",
    "        if results[idx] != 'O' or word == '' or word == '[UNK]':\n",
    "            continue\n",
    "        if word == '[UNK]' :\n",
    "            continue\n",
    "        # Now condition check\n",
    "        if ne_idx >= 0 : \n",
    "            nw_word = ne_lists[ne_idx]['form'][ne_label:]\n",
    "        else :\n",
    "            nw_word = ''\n",
    "\n",
    "        # I-tag condition\n",
    "        if (len(nw_word) > 0) & (nw_word.startswith(word)) & (results[idx-1][0] == 'B' or results[idx-1][0] == 'I') :\n",
    "            results[idx] = 'I-' + ne_lists[ne_idx]['label'][:2]\n",
    "            ne_label += len(word)\n",
    "        else : # B-tag condition\n",
    "            back_idx = ne_idx\n",
    "            back_label = ne_label\n",
    "            while ne_idx + 1 < ne_cnt :\n",
    "                ne_idx += 1\n",
    "                ne_label = 0\n",
    "                nw_word = ne_lists[ne_idx]['form']\n",
    "                if (len(nw_word) > 0) & (nw_word.startswith(word)) :\n",
    "                    results[idx] = 'B-' + ne_lists[ne_idx]['label'][:2]\n",
    "                    ne_label += len(word)\n",
    "                    break\n",
    "            if ne_idx + 1 == ne_cnt and ne_label == 0:\n",
    "                ne_idx = back_idx\n",
    "                ne_label = back_label\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁태', '안', '군의', '회', ',', '▁20', '19', '년', '‘', '군', '민', '중심', '’', '의', '정', '성과', '▁빛', '났다', '!', '[SEP]']\n",
      "['[CLS]', 'B-OG', 'I-OG', 'I-OG', 'I-OG', 'O', 'B-DT', 'I-DT', 'I-DT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"태안군의회, 2019년‘군민중심’의정성과 빛났다!\"\n",
    "ne = [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"form\": \"태안군의회\",\n",
    "            \"label\": \"OGG_POLITICS\",\n",
    "            \"begin\": 0,\n",
    "            \"end\": 5\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"form\": \"2019년\",\n",
    "            \"label\": \"DT_YEAR\",\n",
    "            \"begin\": 7,\n",
    "            \"end\": 12\n",
    "        }\n",
    "]\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "tok = tokenizer(sentence, padding=True, truncation=True) # Has Input_ids, token_type_ids, attention_mask\n",
    "tokens_word = tokenizer.convert_ids_to_tokens(tok['input_ids'])\n",
    "print(tokens_word, tagging(tokens_word, ne), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Json loads & dataframe preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_files(path='./dataset/NLNE2202211219.json') :\n",
    "    with open(path, \"r\") as f :\n",
    "        bef_data = json.load(f)\n",
    "\n",
    "    bef_data = bef_data['document']\n",
    "\n",
    "    df_tot = pd.DataFrame(columns=['form', 'NE'])\n",
    "\n",
    "    for r in bef_data :\n",
    "        df_tot = df_tot.append(pd.DataFrame.from_records(r['sentence'], columns=['form', 'NE']))\n",
    "\n",
    "    df_tot.dropna(how='any', inplace=True)\n",
    "\n",
    "    return df_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_df(data: List[Any]) -> pd.DataFrame:\n",
    "#     return pd.DataFrame.from_records(data['sentence'], columns=['form'])\n",
    "# ex = bef_data[0]\n",
    "# ex = ex['sentence']\n",
    "# print(type(ex), ex[1], sep='\\n')\n",
    "# df = pd.DataFrame.from_records(ex, columns=['form', 'NE'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Define DataLoader, with tokenizer\n",
    "# Have to define collect_fn, to gather attention mask and another information\n",
    "# tok = tokenizer('뽀로로는 남극에 사는 펭귄이 아니다.', padding=True, truncation=True) # Has Input_ids, token_type_ids, attention_mask\n",
    "# tokenizer.convert_ids_to_tokens(tok['input_ids'])\n",
    "df = load_files()\n",
    "texts = df['form'].to_list()\n",
    "ne = df['NE'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset) :\n",
    "    def __init__(self, texts, labels, tokenizer, max_len = 256) -> None:\n",
    "        self.tokenizer = tokenizer \n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index) -> Any:\n",
    "        # tokenizer\n",
    "        input = self.texts[index]\n",
    "        sentence = self.tokenizer(input, padding = 'max_length', truncation = True, max_length = self.max_len) # Input_ids, token_type_ids, attention_mask\n",
    "        tags = tagging(self.tokenizer.convert_ids_to_tokens(sentence['input_ids']), self.labels[index])\n",
    "        convert_tags = [-100 if i in ['[CLS]', '[SEP]', '[PAD]'] else label_to_idx[i] for i in tags]\n",
    "        return {\n",
    "            'sentence' : input, # str\n",
    "            'input_ids' : torch.tensor(sentence['input_ids'], dtype=torch.long).to(devices),\n",
    "            'token_type_id' : torch.tensor(sentence['token_type_ids'], dtype=torch.long).to(devices),\n",
    "            'attention_mask' : torch.tensor(sentence['attention_mask'], dtype=torch.long).to(devices),\n",
    "            'labels' : torch.tensor(convert_tags, dtype=torch.long).to(devices)\n",
    "        } # Collect_fn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, test_texts, train_ne, test_ne = train_test_split(texts, ne, test_size=0.2, random_state=42)\n",
    "test_dataset = CustomDataset(test_texts, test_ne, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "32\n",
      "5\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for batch in test_loader :\n",
    "    if cnt > 1 :\n",
    "        break\n",
    "    cnt += 1\n",
    "    print(len(batch), len(batch['sentence']), sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_implement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
