{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "* Python 3.7.12, Pytorch 1.10 + cu113\n",
    "* AMD 5900X, RTX 3090Ti, Local Jupyter Notebook\n",
    "* To install Kobert, use the following command  \n",
    "```pip install git+https://git@github.com/SKTBrain/KoBERT.git@master kobert-transformers ```\n",
    "* We use ```Accelerator``` by ```Huggingface``` in this project, so gpu needs essentially. (Can single GPU!)\n",
    "* **Not to be confused**, for crf, there is pytorch-crf and TorchCRF. The library used in this project is TorchCRF.\n",
    "* (Not for team projects) If you have any problems running the code after installing the library, please leave an issue.\n",
    "\n",
    "# Datafile\n",
    "* We use data ```./dataset/NLNE2202211219.json``` by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Default Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, TrainingArguments, get_linear_schedule_with_warmup\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from process import CustomDataset, collect_fn_bert, tagging, load_files, collect_fn\n",
    "from bert_model import NERBertCRF, NERBertSVM\n",
    "from svm import KernelSVM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from kobert_transformers import get_tokenizer, get_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_init() :\n",
    "    label_list = ['PS', 'FD', 'TR', 'AF', 'OG', 'LC', 'CV', 'DT', 'TI', 'QT', 'EV', 'AM', 'PT', 'MT', \"TM\"] # Number of labels: 15\n",
    "    label_fin = []\n",
    "    label_fin += ['B-' + i for i in label_list]\n",
    "    label_fin += ['I-' + i for i in label_list]\n",
    "    label_fin += ['O']\n",
    "    label_fin += ['[CLS]']\n",
    "    label_fin += ['[SEP]']\n",
    "    label_fin += ['[PAD]']\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(label_fin)}\n",
    "    idx_to_label = {idx: label for idx, label in enumerate(label_fin)}\n",
    "    return label_fin, label_to_idx, idx_to_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training Part, Bert\n",
    "* At the end of the model, you can decide whether to use SVM or CRF.\n",
    "* This is controlled by a variable called ```model_base```, and if you want to use an SVM, you can store an SVM in it, otherwise a CRF.\n",
    "* In addition to the model architecture, if you want to change the training arguments, you can modify training_args.\n",
    "    * However, due to the current low version of Transformer, some of the factors provided in the current documentation may not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model_base = \"CRF\"\n",
    "traing_args = TrainingArguments(\n",
    "    output_dir=f\"./checkpoints/Bert_{model_base}_checkpoints\",\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=12,\n",
    "    warmup_ratio=0.1,\n",
    "    warmup_steps=0,\n",
    "    # **default_args\n",
    ") # Optional\n",
    "\n",
    "PATH = traing_args.output_dir\n",
    "\n",
    "if not os.path.exists(PATH) :\n",
    "    os.makedirs(PATH)\n",
    "\n",
    "df = load_files() # Have to control path (argparser)\n",
    "texts = df['form'].to_list()\n",
    "ne = df['NE'].to_list()\n",
    "\n",
    "train_texts, test_texts, train_ne, test_ne = train_test_split(texts, ne, test_size=0.2, random_state=42) # fix training dataset\n",
    "train_dataset = CustomDataset(train_texts, train_ne)\n",
    "test_dataset = CustomDataset(test_texts, test_ne)\n",
    "train_loader = DataLoader(train_dataset, batch_size = traing_args.per_device_train_batch_size, shuffle=True, collate_fn=collect_fn_bert)\n",
    "test_loader = DataLoader(test_dataset, batch_size = traing_args.per_device_eval_batch_size, shuffle=True, collate_fn=collect_fn_bert)\n",
    "\n",
    "# Prepare Model, optimizer\n",
    "if model_base == \"CRF\" :\n",
    "    model = NERBertCRF()\n",
    "else :\n",
    "    model = NERBertSVM()\n",
    "\n",
    "decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
    "        \"weight_decay\": traing_args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr= traing_args.learning_rate, eps= traing_args.adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, traing_args.warmup_steps, len(train_loader) * traing_args.num_train_epochs)\n",
    "accelerator = Accelerator(gradient_accumulation_steps=traing_args.gradient_accumulation_steps)\n",
    "model, train_loader, test_loader, optimizer, scheduler = accelerator.prepare(model, train_loader, test_loader, optimizer, scheduler)\n",
    "\n",
    "running_loss = 0.0\n",
    "correct_label_wr, total_label_wr = 0, 0\n",
    "\n",
    "writer = SummaryWriter('./runs/bert/' + model_base)\n",
    "\n",
    "for epoch in range(traing_args.num_train_epochs) :\n",
    "    model.train() \n",
    "    with tqdm(train_loader, unit=\"batch\") as pbar:\n",
    "        for i, data in enumerate(pbar) :\n",
    "            optimizer.zero_grad()\n",
    "            pbar.set_description(f\"Epoch {epoch+1}\")\n",
    "            batch_train, batch_label = data['texts'], data['labels']\n",
    "            with accelerator.accumulate(model) :\n",
    "            # TODO: Seperate SVM & CRF\n",
    "                now_loss, batch_labels, pad_output = model(batch_train, batch_label) # Output\n",
    "                if model_base == \"CRF\" :\n",
    "                    now_loss *= -1\n",
    "                    running_loss += now_loss.mean()\n",
    "                else :\n",
    "                    running_loss += now_loss\n",
    "                accelerator.backward(now_loss.mean())\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "            correct_label_wr += torch.sum((pad_output == batch_labels) & (batch_labels != 33) & (batch_labels != 30))\n",
    "            total_label_wr += torch.sum((batch_labels != 33) & (batch_labels != 30))\n",
    "            if i % 100 == 99 :\n",
    "                writer.add_scalar('train/training_loss',\n",
    "                                  running_loss / 1000,\n",
    "                                  epoch * len(train_loader) + i + 1)\n",
    "                running_loss = 0.0\n",
    "\n",
    "            pbar.set_postfix(loss = now_loss.mean().item(), correct_label = correct_label_wr.item(), total_label = total_label_wr.item())\n",
    "    # One epoch ends\n",
    "    accuracy = correct_label_wr / total_label_wr\n",
    "    writer.add_scalar('train/accuracy',\n",
    "                        accuracy.item(),\n",
    "                        epoch + 1)\n",
    "    writer.add_scalar('train/correct labels',\n",
    "                        correct_label_wr.item(),\n",
    "                        epoch + 1)\n",
    "    correct_label_wr, total_label_wr = 0, 0\n",
    "    # Save state\n",
    "    # accelerator.save_model(model, traing_args.output_dir)\n",
    "    state = {\n",
    "        'epoch' : epoch,\n",
    "        'state_dict' : model.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "        'loss' : now_loss\n",
    "    }\n",
    "    torch.save(state, PATH + model_base + f\"{epoch+1}_edit.pkl\")\n",
    "    # Validation Part\n",
    "    print(\"==========Validation Start==========\")\n",
    "    correct_val, totals_val = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad() :\n",
    "        for data in test_loader :\n",
    "            batch_test, batch_test_label = data['texts'], data['labels']\n",
    "            _, batch_val_labels, val_pad_output = model(batch_test, batch_test_label)\n",
    "            correct_val += torch.sum((val_pad_output == batch_val_labels) & (batch_val_labels != 33) & (batch_val_labels != 30))\n",
    "            totals_val += torch.sum((batch_val_labels != 33) & (batch_val_labels != 30))\n",
    "    accuracy_val = correct_val / totals_val\n",
    "    print(f\"Correct labels: {correct_val.item()}, total labels: {totals_val.item()}, so accuracy is: {accuracy_val.item()}\")\n",
    "    writer.add_scalar('val/validation_accuracy',\n",
    "                      accuracy_val.item(),\n",
    "                      epoch + 1)\n",
    "    writer.add_scalar('val/validation_labels',\n",
    "                      correct_val.item(),\n",
    "                      epoch + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Model & dataset Loading for Inference\n",
    "* If you don't train, then please use these cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_base = \"Bert_CRF\"\n",
    "# PATH = f\"./checkpoints/{model_name}_checkpoints/temps/\"\n",
    "checkpoint_num = 12 # Change checkpoint\n",
    "\n",
    "PATH += f\"{model_base[-3:]}{str(checkpoint_num)}_edit.pkl\" # If other path, have to change\n",
    "\n",
    "print(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "# df = load_files() # Have to control path (argparser)\n",
    "# texts = df['form'].to_list()\n",
    "# ne = df['NE'].to_list()\n",
    "\n",
    "# # tokenizer\n",
    "# train_texts, test_texts, train_ne, test_ne = train_test_split(texts, ne, test_size=0.2, random_state=42) # fix training dataset\n",
    "# test_dataset = CustomDataset(test_texts, test_ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traing_args = TrainingArguments(\n",
    "#     output_dir=f\"./checkpoints/Bert_{model_name[-3:]}_checkpoints/\",\n",
    "#     per_device_train_batch_size=32,\n",
    "#     gradient_accumulation_steps=8,\n",
    "#     learning_rate=5e-5,\n",
    "#     weight_decay=0.01,\n",
    "#     num_train_epochs=12,\n",
    "#     warmup_ratio=0.1,\n",
    "#     warmup_steps=0,\n",
    "#     # **default_args\n",
    "# ) # Optional\n",
    "checkpoint = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Inference & Export results by csv (Total label, correct label, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle=True, collate_fn=collect_fn_bert)\n",
    "\n",
    "if model_base == \"CRF\" :\n",
    "    model = NERBertCRF()\n",
    "else :\n",
    "    model = NERBertSVM()\n",
    "decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
    "        \"weight_decay\": traing_args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr= traing_args.learning_rate, eps= traing_args.adam_epsilon)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "accelerator = Accelerator()\n",
    "model, test_loader, optimizer = accelerator.prepare(model, test_loader, optimizer)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer label\n",
    "idx_to_label, label_to_idx = model.idx_to_label, model.label_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_test, totals_test = 0, 0\n",
    "totals_labels = [0] * len(label_to_idx)\n",
    "correct_labels = [0] * len(label_to_idx)\n",
    "with torch.no_grad() :\n",
    "    for _, data in enumerate(tqdm(test_loader)) :\n",
    "        batch_test, batch_test_label = data['texts'], data['labels']\n",
    "        _, batch_val_labels, val_pad_output = model(batch_test, batch_test_label)\n",
    "        correct_test += torch.sum((val_pad_output == batch_val_labels) & (batch_val_labels != 33) & (batch_val_labels != 30))\n",
    "        correct_labels_c = torch.where(val_pad_output == batch_val_labels, val_pad_output, 33)\n",
    "        totals_labels = [sum(x).item() for x in zip(totals_labels, torch.bincount(batch_val_labels.reshape(-1)))]\n",
    "        correct_labels = [sum(x).item() for x in zip(correct_labels, torch.bincount(correct_labels_c.reshape(-1)))]\n",
    "        totals_test += torch.sum((batch_val_labels != 33) & (batch_val_labels != 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_correct, useful_totals = correct_labels[:30], totals_labels[:30]\n",
    "print(len(useful_correct))\n",
    "temp = []\n",
    "for i, (c, t) in enumerate(zip(useful_correct, useful_totals)) :\n",
    "    print(f\"labels: {idx_to_label[i]} / correct labels: {c} with total lebels: {t} => accuracy: {c/t if t != 0 else 0}\")\n",
    "    temp.append([idx_to_label[i], c, t, c/t if t!=0 else 0])\n",
    "df = pd.DataFrame(temp, columns=[\"label_name\", \"correct_label\", \"total_label\", \"accuracy\"])\n",
    "df.to_csv(f\"results_{model_base}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cose362",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
